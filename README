COMPILATION:
============

You need to have a recent version of the Boost C++ libararies
(www.boost.org) installed, basically the sub-part providing
"boost-program-options" (>= version 1.34), including the development header
files.

You also need a recent version of the SQLite (>= version 3) libraries
(www.sqlite.org) which is used to load large mapping files without flooding
your main memory. Compilation requires the sqlite3 development header files.

Finally, you will need the build management tool "cmake" >= version 2.6
and the UNIX tools "make" and "uname" which should be contained in a standard
installation.

On a Unix/Linux box type
./build.sh

and all programs will be built in a sub-directory called "Build-ARCH" where
ARCH is your computer architecture.

--------------------------------------------------------------------------------

INSTALLATION:
=============

There is no installation procedure. Either copy the executables from the build
directory to your prefered directory in your PATH variable like /usr/local/bin
or execute them by prefixing them with the build directory.

--------------------------------------------------------------------------------

USAGE:
======

Basic concept:
The basic pipeline concept allows splitting the overall prediction into
individual sub-tasks that are implemented in separate programs. Intermediate
data is text-based and consists of lines with TAB-separated fields. At each step
of the process the user can decide whether to save an intermediate file, e. g.
compressing it using a standared compressor like gzip or bzip2, or to feed it
directly into the next program without the necessity to save the data on a slow
disk storage. A binning process is as follows:

--------------------------------------------------------------------------------
|        ALIGN => CONVERT [=> PREFILTER] => PREDICT SEGMENTS [=> BINNING       |
--------------------------------------------------------------------------------

(1) ALIGN: Align sample sequences against database.

The aligner can be replaced as long a conversion tool is provided that will
convert the aligners output into the universal alignment format used in the
pipeline. Aligners supported out-of-the-box are

NCBI blastn
NCBI blastx
lastal

--------------------------------------------------------------------------------

(2) CONVERT:   Transform alignment into universal TAB-separated format.

The intermediate format is quite simple, a tab-separated text file.
We provide scripts for use with "NCBI BLAST" (blastn or blastx using XML output)
and "LAST" (using maf output) in the alignment phase.

blastxml2alignments.py:
Blast+ XML output converter (use "-outfmt 5"). Check out the options using the
"-h" option of blastn/blastx.

lastmaf2alignments.py:
LAST MAF format converter (use "-f 1"). This version now works with the main
stream version of LAST. As a limitation of the MAF format used by LAST
(http://genome.ucsc.edu/FAQ/FAQformat.html), sequence identifiers of database
and query sequences must not contain white spaces or the first white space
separated part must be uniquely trackable for you because the rest will be
discarded when reporting the alignments. LAST output is not sorted by query
identifier, so you must run through "sort" after conversion to intermediate
format. The UNIX sort command like sort from the GNU coreutils is usually
sufficient for this.

--------------------------------------------------------------------------------

(3) PREFILTER: Apply e. g. quality restriction filters to exclude bad
               alignments. This step is optional and ususally not but can help
               you to implement your quality policies or reduce run-time.

alignment-prefilter:
Support some filtering schemes based on absolute and relative score values or
e-values. Check out the options.

alignment-prefilter-training:
Filtering in the case when the taxonomic origin of
the query sequences are known (accession number is contained in the sequence
identifier). This helps to evaluate the algorithms.

--------------------------------------------------------------------------------

(4) PREDICT SEGMENTS:   Make a taxonomic classification for regions of homology
                        on the sample sequences. Depending on the algorithm,
                        and options, there can be multiple such regions on a
                        contiguous stretch of sequence.

taxator:
Does the actual taxonomic prediction by mapping the alignment reference
sequences on the NCBI taxonomy tree. taxator-tk implements its own re-alignment
placement algorithm (RPA) of taxonomic classification of sequence segments that
is based on pairwise alignments and strikes to approximate placement
via phylogenetic methods. This method requires some re-calculation but will give
more conservative results aiming at a lower number of false predictions. As a
general and fast way the LCA as used in MEGAN algorithm is also implemented and
refined to use partially classified taxa in the NCBI taxonomy tree.

--------------------------------------------------------------------------------

(5) EVALUATE:  In the case of labeled sample sequences, evaluation is possible.

prediction2distances:
Converts a taxonomic prediction of a sequence with known taxonomic origin into
taxonomy tree distances.

prediction-error:
Converts a triple of taxonomy tree distances into an error quantity. Some
different formulas are implemented.

--------------------------------------------------------------------------------

Extra tools:
------------

rank-filter:                    Traverses taxonomic ids up to a given rank.
name-filter:                    Prints the name or rank name of a given taxon.
acc2taxid:                      Convert sequence identifier to taxonomic ID.
extract-fastacomment-ncbifield: Parse NCBI FASTA file headers fields.

--------------------------------------------------------------------------------

NCBI taxonomy:
--------------

All programs that need a taxonomy to work will read the NCBI taxonomy dump files
from a directory that is defined via the environment variable
TAXATORTK_NCBI_TAXONOMY. You can use the provided script
"download-ncbi-taxonomy-gimapping.sh" to download the taxonomy files from the
official NCBI FTP server. Example:

mkdir ncbi_taxdump
cd ncbi_taxdump
path_where_you_unpacked_source/tools/download-ncbi-taxonomy-gimapping.sh
export TAXATORTK_NCBI_TAXONOMY=$PWD


Examples:
---------

For database construction with LAST, use the accession number to index the
sequences. LAST README shows how to transform your FASTA identifiers.

(a)
# Align sequences to DATABASE using LAST and do a best-hit classification
# (reference sequence identifiers are the NCBI fasta headers)

lastal -f 1 DATABASE mysample.fna | last2alignments.py | sort | taxator -a n-best-lca -n 1 -g gitaxid.mapping

(b)
# Do a BLAST-MEGAN-style classification with blastn alignments and with a
top-percent filter value of 30 % and a minimum E-value of 0.01.

blastn -task blastn -db DATABASE -outfmt 5 -query mysample.fna | blastxml2alignments.py | taxator -a megan-lca -t 0.3 -e 0.01 -g gitaxid.mapping

# In this case it is better to restict the evalue directly in the BLAST search.

blastn -task blastn -db DATABASE -outfmt 5 -query mysample.fna -evalue 0.01 | taxator -a megan-lca -t 0.3 -g gitaxid.mapping

(c)
# If you want to save the alignments with LAST compressed and in sorted order

lastal -f 1 DATABASE mysample.fna | lastmaf2alignments.py | sort | gzip > dump.alignments.gz

# and continue with a RPA prediction and saving the predictions into a file
zcat dump.alignments.gz | taxator -g gitaxid.mapping -a rpa [missing options] -q query.fna -b reference.fna -g gitaxid.mapping > dump.predictions

(d)
# Show the corresponding predictions with taxon names
name-filter -f 2 -s name < dump.predictions

# or if you want to show the names on the phylum level.
rank-filter -r phylum -f 2 < dump.predictions | name-filter -f 2 -s name

(e)
# Using e-values with LAST is more tricky because it requires a second binary.
# This didn't seem to have a strong effect on classification though.

lastal -f 1 DATABASE mysample.fna | lastex -z 1 DATABASE.prj mysample.prj - | lastmaf2alignments.py | sort | ...

(f)
# There is a downstream tool called binner that will use the segment predictions
# to bin a sample

binner < dump.predictions > dump.binning

(g)
# Putting together above commands one could write the following workflow

lastal -f 1 DATABASE mysample.fna | lastmaf2alignments.py | sort > sample.alignments
taxator -g gitaxid.mapping -a rpa [missing options] -q query.fna -b reference.fna -g gitaxid.mapping < sample.alignments > sample.predictions
binner < sample.predictions > sample.binning

--------------------------------------------------------------------------------

RECOMMENDATION
==============

For comparable settings, use BLAST and LAST with the same scoring scheme, use
BLAST raw scores for classification instead of bitscores. If you want to have
precise scores, you need to recalculate the BLAST raw scores (implemented in the
conversion script "blastxml2alignments.py"), because recent BLAST versions do
not give precise scores as LAST does. The LAST aligner does not produce E-values
(only through separate program).

--------------------------------------------------------------------------------

TAXONOMIC PLACEMENT ALGORITHMS
==============================

lca:

megan-lca:

rpa:

--------------------------------------------------------------------------------

TECHNICAL ASPECTS
=================

The software implementation and design try to be as slick, fast and flexible as
possible to provide a real tool kit for building an analysis pipeline or to be
integrated in such. The following considerations were taken:

- the mapping from sequence identifiers to taxonomic IDs are usually small
enough to be read into memory. I recommend to generate a custom mappings file
by filtering out all identifiers existing in the DB from the large mappings that
you can usually download from FTP servers. If you want however, you can instead
use the SQLite3 backend that is slower but does not flood you memory.

- all tree operations implemented, especially the lowest common ancestor finding
are implemented in constant time.

- the taxonomy tree is read from the NCBI dump files that define the NCBI
taxonomy each time a module is loaded. This overhead may be reduced in future
versions.

- the RPA algorithms depends on sequence random access and thus loads the query
and reference in memory to be as fast as possible at the moment. This can
require large amounts of memory for reference collections like RefSeq and can
usually only be done on server hardware. We work towards efficient on-disk
sequence access that will allow the RPA to run on consumer hardware.
